{"meta":{"title":"Chance Li 的博客","subtitle":null,"description":null,"author":"Chance Li","url":"http://www.ilmadyna.com"},"pages":[{"title":"书单","date":"2018-05-12T00:50:54.425Z","updated":"2018-05-11T08:43:46.357Z","comments":false,"path":"books/index.html","permalink":"http://www.ilmadyna.com/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2018-05-12T09:17:26.746Z","updated":"2018-05-12T09:17:26.746Z","comments":false,"path":"about/index.html","permalink":"http://www.ilmadyna.com/about/index.html","excerpt":"","text":"数据分析菜鸟，通过学习希望成为真正的数据分析师。"},{"title":"分类","date":"2018-05-12T00:50:54.439Z","updated":"2018-05-11T08:43:46.357Z","comments":false,"path":"categories/index.html","permalink":"http://www.ilmadyna.com/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-05-12T00:50:54.466Z","updated":"2018-05-11T08:43:46.358Z","comments":false,"path":"repository/index.html","permalink":"http://www.ilmadyna.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-05-12T00:50:54.453Z","updated":"2018-05-11T08:43:46.357Z","comments":true,"path":"links/index.html","permalink":"http://www.ilmadyna.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-05-12T00:50:54.480Z","updated":"2018-05-11T08:43:46.358Z","comments":false,"path":"tags/index.html","permalink":"http://www.ilmadyna.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"清华同方大数据部","slug":"清华同方大数据部","date":"2018-05-14T15:55:00.092Z","updated":"2018-05-15T03:23:10.890Z","comments":true,"path":"2018/05/14/清华同方大数据部/","link":"","permalink":"http://www.ilmadyna.com/2018/05/14/清华同方大数据部/","excerpt":"","text":"公司简介同方介绍1997年6月25日，清华同方股份有限公司正式成立。 1997年6月27日，清华同方在上海证券交易所上市（股票代码：600100）。 2006年5月30日，清华同方股份有限公司更名为“同方股份有限公司”。 以“科技服务社会”为宗旨，同方股份有限公司紧密依托清华大学的科研实力与人才平台，围绕“技术＋资本”、“合作与发展”、“品牌化+国际化”的发展战略，弘扬“承担、探索、超越；忠诚、责任与价值等同”的企业文化，在信息和能源环境两大产业领域中不断探索、创新。 近年来，公司贯彻“以信息、安防、节能环保等三大科技产业为主业，以金融投资和科技园建设为两翼，主业突出，两翼齐飞”的“一主两翼”发展方针，一方面按产业链孵化和培育了智能芯片、计算机、数字城市、大数据应用、多媒体、移动互联、知识网络、大军工、大安全、半导体与照明、环境科技、节能环保等与国家发展、国计民生密切相关的十二大主干产业集群，并在北京、河北、辽宁、江苏、江西等多地建成了与产业配套的科技园区；另一方面，发展金融产业，截至目前，公司旗下已有同方泰德（港股代码：01206）、同方国芯（股票代码：002049）、泰豪科技（股票代码：600590）等多家上市公司。 截至2016年，同方股份有限公司总资产超过570亿元，年营业收入近300亿元。每年，同方股份申请的中国及海外专利、计算机软件著作权登记累计均达到四千余项，累计获得国家及省部级科学技术奖百余项，承担国家科技攻关项目和科技重大专项超过300项，历年入选“中国电子信息百强”、“中国制造业企业500强”、“中国企业信用100强”，“中国电子信息行业创新能力五十强企业”，被评为“国家高新技术企业”、“国家信息系统集成及服务大型一级企业”。“国家安全可靠计算机信息系统集成重点企业”、“中关村国家自主创新示范区首批十百千工程重点培育企业”。 本部介绍大数据产业本部（原物联网应用产业本部）成立于2010年5月，是同方股份有限公司核心产业之一 面向中国智慧建设领域，大数据产业本部以发展同方软件产业为主导，以大数据产业发展为核心，为政府和行业提供智慧应用系统解决方案、系统集成及产品技术服务;以创造数据价值,创新社会服务为本部使命，致力于成为大数据时代的领引者，中国智慧建设的卓越服务商。 公司主营业务聚焦于大数据技术应用服务相关的各个行业领域。基于多年来在数据业务领域的实践，按照数据本质规律，在业内率先提出 “大数据成就智慧”的创新理念，开发出了具有自主知识产权，以大数据分析与决策支持平台和视频大数据人工智能应用平台为代表的一系列颠覆性大数据智慧应用产品。前瞻性的提出建立跨部门、跨行业、跨领域的“三跨一体化”大数据智慧应用管理思想，力求通过丰富的项目实践，实现城市运行、管理及发展中的科学决策和有效落实，保证城市各个机构业务的高效协同、信息的广泛共享和产业的健康发展。 本部多年来荣获荣誉近百项，“优秀大数据应用服务商奖”、“中国大数据最佳创新产品奖”、“中国软件行业最具影响力奖”、“中国软件行业最佳解决方案奖”、“十佳IT服务商”、 “中国软件创新奖”等。连续多年被评为北京市信用企业，北京市五部委联合认定“中关村国家自主创新区新产品新技术认定”等多项荣誉。 企业文化承担、探索、超越、忠诚、责任与价值等同，同方大数据产业本部在秉承同方企业文化的同时，积极探索、寻求共同的理想、基本价值观、生活习惯和行为规范。 我们一直把企业文化建设作为重中之重，大数据产业本部的企业文化与同方的企业文化“承担、探索、超越、忠诚、责任与价值等同”一脉相承。 愿景大数据时代的领引者，中国智慧建设的卓越服务商 使命创造数据价值，创新社会服务 战略专业团队 专注领域 专家技术 大数据成就智慧 核心价值观颠覆、创新、协同、前行 颠覆：是一种以目标为导向的决心 是一种改变和突破自我的心态 是努力将不可能变成可能的勇气 是敢于打破固定思维的精神 创新：是创造性善于落地的行动 是实践能力不断更新的突破 是新市场环境中认知能力的挑战 是踏实务实的锐意进取 协同：是大局意识和奉献精神的胸怀 是保证组织高效运转的基石 是保障团队快速发展的要求与纪律 是实现1+1大于2效果的原因 前行：是一切努力的最终目标 是不断逆水行舟，不进则退的鞭策 是始终保持创业者拼搏进取的心态 是个人与企业共同成长的愿景 核心业务业务定位 核心技术大数据（DT）采集分析与支撑决策平台针对发改委、经信委、大数据局、统计局，在经济调控、城镇化建设、产业监测、规划融合等问题上，将相关目标、政策等内容充分融合，利用统计部门、行业及社会互联网数据，进行深度融合，随时开展多角度多层次的探索分析、数据漫游服务，以求破解问题根源和相互作用，为政府提供一个各方面各层次管理决策分析的支撑服务平台确保政府用数据说话、管理、决策、创新机制的有效落地，从而实现经济社会在新常态下健康高速发展。 视频大数据与人工智能应用平台根据前端摄像机采集的实时视频图像数据或已有视频图像数据，采用突变检测、深度学习相关技术，通过算法模型对视频图像进行分析，对实时视频流进行实时监视、识别、记录、检测、报警，并及时将分析结果通过报警提示发送给监管人员进行应急处理；系统主要从视频分析和应用两大类功能进行设计和开发，能够快速检测异常行为和异常事件，实时产生报警，帮助监测人员快速响应事件，提高视频事件的处理效率；而24小时的实时视频分析远远超越了人为监视画面的监控效率，真正达到 “实时监控，即时预警”的目的。 业务全景 解决方案智慧发改针对发改委、经信委、大数据局、统计局，在经济调控、城镇化建设、产业监测、规划融合等问题上，将相关目标、政策等内容充分融合，利用统计部门、行业及社会互联网数据，进行深度融合分析运算，以图形报表、GIS及全尺度三维动画等可视化手段进行展示。同时根据领导关注问题，随时开展多角度多层次的探索分析、数据漫游服务，以求破解问题根源和相互作用，为政府提供一个各方面各层次管理分析的支撑服务平台，确保政府用数据说话、管理、决策、创新机制的有效落地，从而实现经济社会在新常态下健康高速发展。 智慧交通同方交通慧眼达 同方省级交通大数据中心 数据城管同方的“数据”城管模式是在数字城管的基础之上，以数据成就智慧为理念，打造数据驱动知识社会创新的城市管理新模式。应用了大数据创新技术填补城管行业空白，推动了视频大数据在城市管理中的应用创新，开创了人工智能在城市管理中的服务创新。 智慧司法同方智慧司法旨在通过快速迭代的、自然演化的技术架构，进口司法行政核心目标，以一厅两局（省司法厅，省监狱管理局，省戒毒管理局）为业务主线，打造全面覆盖五级（司法部、省、市州、县区、乡镇）司法行政业务指标体系，构筑全域体系化、资源化、高价值的司法行政大数据资源体系，建设“大整合、高共享、全规范“的“一站式”司法行政综合管理平台。 智慧信用","categories":[{"name":"行业知识","slug":"行业知识","permalink":"http://www.ilmadyna.com/categories/行业知识/"},{"name":"公司介绍","slug":"行业知识/公司介绍","permalink":"http://www.ilmadyna.com/categories/行业知识/公司介绍/"}],"tags":[{"name":"业务知识","slug":"业务知识","permalink":"http://www.ilmadyna.com/tags/业务知识/"}]},{"title":"机器学习之算法","slug":"机器学习之算法","date":"2018-05-14T14:53:21.998Z","updated":"2018-05-14T15:04:37.923Z","comments":true,"path":"2018/05/14/机器学习之算法/","link":"","permalink":"http://www.ilmadyna.com/2018/05/14/机器学习之算法/","excerpt":"","text":"决策树原理顾名思义，决策树就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如CART只能是二叉树），也可以是多叉树（比如ID3、C4.5可以是多叉树或二叉树）。 根节点包含整个样本集，每个叶节点都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。 举个例子： 就像上面这个例子，训练集由三个特征：outlook(天气)，humidity（湿度），windy（是否有风）。那么我们该如何选择特征对训练集进行划分那？连续型特征（比如湿度）划分的阈值又是如何确定的那？ 决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种： （1）当前节点包含的样本属于同一类别，无需划分 （2）当前属性集为空，或所有样本在属性集上取值相同，无法划分 （3）当前节点包含样本集合为空，无法划分 ID3、C4.5、CART这三个是非常著名的决策树算法。简单粗暴来说，ID3使用信息增益作为选择特征的准则；C4.5使用信息增益比作为选择特征的准则；CART使用Gini指数作为选择特征的准则。 ID3熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。 信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。 ID3仅仅适用于二分类问题。ID3仅仅能够处理离散属性。 C4.5C4.5克服了ID3仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 /划分前熵 选择信息增益比最大的作为最优特征。 C4.5处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。 CARTCART与ID3，C4.5不同之处在于CART生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。 CART的全称是分类与回归树。从这个名字中就应该知道，CART既可以用于分类问题，也可以用于回归问题。 回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。 分类树种，使用Gini指数最小化准则来选择特征并进行划分； Gini指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。 信息增益vs信息增益比之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。 Gini指数vs熵既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？ Gini指数的计算不需要对数运算，更加高效Gini指数更偏向于连续属性，熵更偏向于离散属性 剪枝决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法。 剪枝分为预剪枝与后剪枝。 预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。 后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。 那么怎么来判断是否带来泛化性能的提升那？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。 总结决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有ID3、C4.5、CART。 特征选择。特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini指数。 决策树的生成。通常是利用信息增益最大、信息增益比最大、Gini指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集。 决策树的剪枝。决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。 随机森林（Random Forest)集成学习方法集成学习（ensemble learning）通过构建并组合多个学习器来完成学习任务。集成学习通过将多个学习器进行结合，常获得比单一学习器显著优越的泛化性能。 根据个体学习器是否是同类型的学习器（由同一个算法生成，比如C4.5,BP等），分为同质和异质。同质的个体学习器又叫做基学习器，而异质的个体学习器则直接成为个体学习器。 原则：要获得比单一学习器更好的性能，个体学习器应该好而不同。即个体学习器应该具有一定的准确性，不能差于弱学习器，并且具有多样性，即学习器之间有差异。 根据个体学习器的生成方式，目前集成学习分为两大类： 个体学习器之间存在强依赖关系、必须串行生成的序列化方法。代表是Boosting。 个体学习器之间不存在强依赖关系、可同时生成的并行化方法。代表是Bagging和随机森林（Random Forest） Bagging前面提到，想要集成算法获得性能的提升，个体学习器应该具有独立性。虽然“独立”在现实生活中往往无法做到，但是可以设法让基学习器尽可能的有较大的差异。Bagging给出的做法就是对训练集进行采样，产生出若干个不同的子集，再从每个训练子集中训练一个基学习器。由于训练数据不同，我们的基学习器可望具有较大的差异。 Bagging是并行式集成学习方法的代表，采样方法是自助采样法，用的是有放回的采样。初始训练集中大约有63.2%的数据出现在采样集中。 Bagging在预测输出进行结合时，对于分类问题，采用简单投票法；对于回归问题，采用简单平均法。 Bagging优点：高效。Bagging集成与直接训练基学习器的复杂度同阶。Bagging能不经修改的适用于多分类、回归任务。包外估计。使用剩下的样本作为验证集进行包外估计（out-of-bag estimate）Bagging主要关注降低方差。（low variance） 随机森林（Random Forest)原理随机森林（Random Forest）是Bagging的一个变体。Ramdon Forest在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。 原来决策树从所有属性中，选择最优属性。Ramdom Forest的每一颗决策树中的每一个节点，先从该节点的属性集中随机选择K个属性的子集，然后从这个属性子集中选择最优属性进行划分。K控制了随机性的引入程度，是一个重要的超参数。 预测：分类：简单投票法回归：简单平均法 优缺点优点：由于每次不再考虑全部的属性，而是一个属性子集，所以相比于Bagging计算开销更小，训练效率更高。 由于增加了属性的扰动，随机森林中基学习器的性能降低，使得在随机森林在起始时候性能较差，但是随着基学习器的增多，随机森林通常会收敛于更低的泛化误差，相比于Bagging。 两个随机性的引入，使得随机森林不容易陷入过拟合，具有很好的抗噪声能力 对数据的适应能力强，可以处理离散和连续的，无需要规范化 可以得到变量的重要性， 基于oob误分类率和基于Gini系数的变化 缺点：在噪声较大的时候容易过拟合 AdaBoostAdaBoost是Boosting的代表，前面我们提到Boosting是集成学习中非常重要的一类串行化学习方法。 BoostingBoosting是指个体学习器之间存在强依赖关系，必须串行序列化生成的集成学习方法。他的思想来源是三个臭皮匠顶个诸葛亮。Boosting意为提升，意思是希望将每个弱学习器提升为强学习器。 工作机制如下： 先从初始训练集中学习一个基学习器 根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注 基于调整后的样本分布来训练下一个基学习器 如此反复，直到基学习器数目达到T，最终将这T个基学习器进行加权结合 对训练样本分布调整，主要是通过增加误分类样本的权重，降低正确分类样本的权重。 Boosting关注的主要是降低偏差。（low bias） AdaBoost原理面临两个问题： （1）在每一轮，如何改变训练数据的概率分布或者权值分布。（也可以重采样，但是AdaBoost没这么做） （2）如何将弱分类器组合成强分类器 AdaBoost的做法： （1）提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮弱分类器的关注。 （2）采用加权多数表决。具体的，加大分类错误率低的分类器的权值，使其在表决中起较大作用，减少分类误差率大的弱分类器的权值，使其在表决中起较小作用。 弱分类器被线性组合成为一个强分类器。 训练目标： 最小化指数损失函数。三部分组成： （1）分类器权重更新公式 （2）样本分布（也就是样本权重）更新公式 （3）加性模型。 最小化指数损失函数 AdaBoost优缺点优点：不改变所给的训练数据，而不断改变训练数据的权值分布，使得训练数据在基本分类器的学习中起不同的作用。这是AdaBoost的一个特点。利用基本分类器的加权线性组合构建最终分类器，是AdaBoost的另一个特点。AdaBoost被实践证明是一种很好的防止过拟合的方法，但至今为什么至今没从理论上证明。 缺点：AdaBoost只适用于二分类问题。 GBDTDT：回归树 Regression Decision TreeGBDT（Gradient Boosting Decision Tree）又叫MART（Multiple Additive Regression Tree）。是一种迭代的决策树算法。 本文从以下几个方面进行阐述： Regression Decision Tree(DT)Gradient Boosting(GB)Shrinkage(算法的一个重要演进分支，目前大部分源码都是基于该版本实现)GBDT适用范围与随机森林的对比 GB：梯度迭代 Gradient BoostingGDBT中的树全部都是回归树，核心就是累加所有树的结果作为最终结果。只有回归树的结果累加起来才是有意义的，分类的结果加是没有意义的。 GDBT调整之后可以用于分类问题，但是内部还是回归树。 这部分和决策树中的是一样的，无非就是特征选择。回归树用的是最小化均方误差，分类树是用的是最小化基尼指数（CART） GB：梯度迭代 Gradient Boosting首先Boosting是一种集成方法。通过对弱分类器的组合得到强分类器，他是串行的，几个弱分类器之间是依次训练的。GBDT的核心就在于，每一颗树学习的是之前所有树结论和的残差。 Gradient体现在：无论前面一颗树的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，那么残差向量都是它的全局最优方向，这就是Gradient。 ShrinkageShrinkage（缩减）是GBDT算法的一个重要演进分支，目前大部分的源码都是基于这个版本的。 核心思想在于：Shrinkage认为每次走一小步来逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易防止过拟合。也就是说，它不信任每次学习到的残差，它认为每棵树只学习到了真理的一小部分，累加的时候只累加一小部分，通过多学习几棵树来弥补不足。 具体的做法就是：仍然以残差作为学习目标，但是对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小0.01-0.001,导致各个树的残差是渐变而不是陡变的。 本质上，Shrinkage为每一颗树设置了一个weight，累加时要乘以这个weight，但和Gradient没有关系。这个weight就是step。跟AdaBoost一样，Shrinkage能减少过拟合也是经验证明的，目前还没有理论证明。 GBDT适用范围GBDT可以适用于回归问题（线性和非线性），相对于logistic regression仅能用于线性回归，GBDT适用面更广。GBDT也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。 GBDT和随机森林GBDT和随机森林的相同点： 都是由多棵树组成 最终的结果都由多棵树共同决定 GBDT和随机森林的不同点： 组成随机森林的可以是分类树、回归树；组成GBDT只能是回归树 组成随机森林的树可以并行生成（Bagging）；GBDT只能串行生成（Boosting） 对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而GBDT则是将所有结果累加起来，或者加权累加起来 随机森林对异常值不敏感，GBDT对异常值非常敏感 随机森林对训练集一视同仁权值一样，GBDT是基于权值的弱分类器的集成 随机森林通过减小模型的方差提高性能，GBDT通过减少模型偏差提高性能 TIP1. GBDT相比于决策树有什么优点泛化性能更好！GBDT的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于0。这样后面就更加专注于那些分错的样本。 2. Gradient体现在哪里？可以理解为残差是全局最优的绝对方向，类似于求梯度。 3. re-sampleGBDT也可以在使用残差的同时引入Bootstrap re-sampling，GBDT多数实现版本中引入了这个选项，但是是否一定使用有不同的看法。原因在于re-sample导致的随机性，使得模型不可复现，对于评估提出一定的挑战，比如很难确定性能的提升是由于feature的原因还是sample的随机因素。 Logistic回归LR原理参数估计LR的正则化为什么LR能比线性回归好？LR与MaxEnt的关系 LR模型原理首先必须给出Logistic分布： u是位置参数，r是形状参数。对称点是(u,1/2)，r越小，函数在u附近越陡峭。 然后，二分类LR模型，是参数化的logistic分布，使用条件概率来表示： 然后，一个事件的几率（odds）：指该事件发生与不发生的概率比值： 对数几率： 那么对于逻辑回归而言，Y=1的对数几率就是： 最终，输出Y=1的对数几率是输入x的线性函数表示的模型，这就是逻辑回归模型。 参数估计在统计学中，常常使用极大似然估计法来估计参数。即找到一组参数，使得在这组参数下，我们数据的似然度（概率）最大。 似然函数： 对数似然函数： 对应的损失函数： 最优化方法逻辑回归模型的参数估计中，最后就是对J(W)求最小值。这种不带约束条件的最优化问题，常用梯度下降或牛顿法来解决 使用梯度下降法求解逻辑回归参数估计 求J(W)梯度：g(w): 更新Wk： $$ W{k+1} = Wk - \\lambda*g(W_k) $$ 正则化正则化为了解决过拟合问题。分为L1和L2正则化。主要通过修正损失函数，加入模型复杂性评估。 正则化是符合奥卡姆剃刀原理：在所有可能的模型中，能够很好的解释已知数据并且十分简单的才是最好的模型。 p表示范数，p=1和p=2分别应用L1和L2正则。 L1正则化。向量中各元素绝对值之和。又叫做稀疏规则算子（Lasso regularization）。关键在于能够实现特征的自动选择，参数稀疏可以避免非必要的特征引入的噪声。 L2正则化。使得每个元素都尽可能的小，但是都不为零。在回归里面，有人把他的回归叫做岭回归（Ridge Regression），也有人叫他“权值衰减”（weight decay） 一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0. 逻辑回归 vs 线性回归首先，逻辑回归比线性回归要好。 两者都属于广义线性模型。 线性回归优化目标函数用的最小二乘法，而逻辑回归用的是最大似然估计。 逻辑回归只是在线性回归的基础上，将加权和通过sigmoid函数，映射到0-1范围内空间。 线性回归在整个实数范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。而逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型。逻辑曲线在z=0时，十分敏感，在z&gt;&gt;0或z&lt;&lt;0处，都不敏感，将预测值限定为(0,1)。逻辑回归的鲁棒性比线性回归要好。 逻辑回归模型 vs 最大熵模型 MaxEnt简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应为二类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。 最大熵原理：学习概率模型的时候，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。 SVM支持向量集虽然咱们的目标是尽可能的不涉及到公式，但是提到SVM就没有办法不涉及到公式推导，因为面试中只要问到SVM，最基本也是最难的问题就是：SVM的对偶问题数学公式推导。所以想学好机器学习，还是要塌下心来，不仅仅要把原理的核心思想掌握了，公式推导也要好好学习才行。：） SVM原理简单粗暴的说：SVM的思路就是找到一个超平面将数据集进行正确的分类。对于在现有维度不可分的数据，利用核函数映射到高纬空间使其线性可分。 支持向量机SVM是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。SVM的学习策略是间隔最大化，可形式化为求解凸二次规划问题。 SVM分为： 线性可分支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习到的一个线性分类器。 线性支持向量机。当训练数据近似线性可分时，通过软间隔最大化，学习到的一个线性分类器。 非线性支持向量机。当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性支持向量机。 上图中，X表示负例，O表示正例。此时的训练数据可分，线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。 支持向量与间隔支持向量：在线性可分的情况下，训练数据样本集中的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。 函数间隔定义如下： yi表示目标值，取值为+1、-1. 函数间隔虽然可以表示分类预测的准确性以及确信度。但是有个不好的性质：只要成倍的改变W和B，虽然此时的超平面并没有改变，但是函数间隔会变大。所以我们想到了对超平面的法向量W施加一些约束，比如规范化，使得间隔确定，这就引出了几何间隔： 支持向量学习的基本思想就是求解能够正确划分训练数据集并且几何间隔最大的分类超平面。 对偶问题为了求解线性可分支持向量机的最优化问题： 将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。 本来的算法也可以求解SVM，但是之所以要用对偶问题来求解，优点是： 一是对偶问题往往更容易求解二是自然引入核函数，进而推广到非线性分类问题说点题外话，这也是面试中会被问到的一个问题：原始问题既然可以求解，为什么还要转换为对偶问题。答案就是上述这两点。由于篇幅的愿意，此处就不在展开数学公式的推导了，大家可以参考《机器学习》与《统计学习方法》 核函数对于在原始空间中不可分的问题，在高维空间中是线性可分的。 对于线性不可分的问题，使用核函数可以从原始空间映射到高纬空间，使得问题变得线性可分。核函数还可以使得在高维空间计算的内积在低维空间中通过一个函数来完成。 常用的核函数有：高斯核、线性核、多项式核。 软间隔线性可分问题的支持向量机学习方法，对现行不可分训练数据是不适用的。所以讲间隔函数修改为软间隔，对于函数间隔，在其上加上一个松弛变量，使其满足大于等于1。约束条件变为： 优缺点缺点：时空开销比较大，训练时间长核函数的选取比较难，主要靠经验 优点：在小训练集上往往得到比较好的结果使用核函数避开了高纬空间的复杂性泛化能力强 利用sklearn实践使用sklearn用决策树来进行莺尾花数据集的划分问题。代码上没有固定随机种子，所以每次运行的结果会稍有不同。 数据集三维可视化： 在Sepal length和Sepal width二维上的可视化： 在Sepal length和Sepal width二维上的可视化： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from sklearn import treefrom sklearn.svm import SVCfrom subprocess import check_callfrom sklearn.datasets import load_irisfrom sklearn.metrics import accuracy_scorefrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.cross_validation import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifier# load datairis = load_iris()# split valid datasetX_train, X_val, y_train, y_val = train_test_split(iris.data, iris.target, test_size=0.25)# decision treedt = tree.DecisionTreeClassifier(criterion=&apos;gini&apos;)dt2 = tree.DecisionTreeClassifier(criterion=&apos;entropy&apos;)dt.fit(X_train, y_train)dt2.fit(X_train,y_train)# accuracyprint &quot;Decision Tree (Gini) Accuracy: &quot;, accuracy_score(y_val, dt.predict(X_val))print &quot;Decision Tree (Entropy) Accuracy: &quot;, accuracy_score(y_val, dt2.predict(X_val))# decision tree visualizationtree.export_graphviz(dt,out_file=&apos;tree.dot&apos;, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) check_call([&apos;dot&apos;,&apos;-Tpng&apos;,&apos;tree.dot&apos;,&apos;-o&apos;,&apos;tree.png&apos;])# Random forestrfc = RandomForestClassifier()rfc.fit(X_train, y_train)print &quot;Random Forest Accuracy: &quot;, accuracy_score(y_val, rfc.predict(X_val))# AdaBoostadaboost = AdaBoostClassifier(n_estimators=300)adaboost.fit(X_train, y_train)print &quot;AdaBoost Accuracy: &quot;, accuracy_score(y_val, adaboost.predict(X_val))# GBDTgbdt = GradientBoostingClassifier()gbdt.fit(X_train, y_train)print &quot;GBDT Accuracy: &quot;,accuracy_score(y_val, gbdt.predict(X_val))# logistic regression# for multiclass use One-vs-Rest(ovr) modellr = LogisticRegression(multi_class=&apos;ovr&apos;)lr.fit(X_train, y_train)print &quot;Logistic Regression Accuracy: &quot;, accuracy_score(y_val, lr.predict(X_val))# SVMsvm = SVC()svm.fit(X_train, y_train)print &quot;SVM Accuracy: &quot;, accuracy_score(y_val, svm.predict(X_val)) Decision Tree可视化，也就是生成的决策树：","categories":[{"name":"技术知识","slug":"技术知识","permalink":"http://www.ilmadyna.com/categories/技术知识/"},{"name":"数据分析","slug":"技术知识/数据分析","permalink":"http://www.ilmadyna.com/categories/技术知识/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://www.ilmadyna.com/tags/数据分析/"},{"name":"算法","slug":"算法","permalink":"http://www.ilmadyna.com/tags/算法/"},{"name":"机器学习","slug":"机器学习","permalink":"http://www.ilmadyna.com/tags/机器学习/"}]},{"title":"数据分析之关联规则Apriori算法","slug":"数据分析之关联规则Apriori算法","date":"2018-05-10T09:02:55.157Z","updated":"2018-05-12T11:07:04.403Z","comments":true,"path":"2018/05/10/数据分析之关联规则Apriori算法/","link":"","permalink":"http://www.ilmadyna.com/2018/05/10/数据分析之关联规则Apriori算法/","excerpt":"","text":"加载数据集这里我们选择使用Groceries数据集进行案例分析，该数据集是某一食品杂货店一个月的真实交易记录，每一行代表一笔交易所购买的产品。 数据查看与筛选R语言中自带一些函数如summary，size，dim，colnames等可以大概查看交易数据集groceries的基本概况1summary(groceries) 运行结果的含义: 第一段:总共有9835条交易记录transaction，169个商品item。density=0.026表示在稀疏矩阵中1的占比。 第二段:最频繁出现的商品item，以及其出现的次数。可以计算出最大支持度。 第三段:每笔交易包含的商品数目，以及其对应的5个分位数和均值的 统计信息。如:2159条交易仅包含了1个商品，1643条交易购买了2件商品，一条交易购买了32件商品。那段统计信息的含义是:第一分位数是2，意味着25%的交易包含不超过2个item。中位数是3表明50%的 交易购买的商品不超过3件。均值4.4表示所有的交易平均购买4.4件商品。 第四段:如果数据集包含除了Transaction Id和Item之外的其他的列(如，发生交易的时间，用户ID等等)，会显示在这里。这个例子，其实没有新的列，labels就是item的名字。 若要查看每个transaction包含item的数目basketSize，使用size()函数 若是计算每个item的支持度(占比)itemFreq，使用 itemFrequency()函数 若要计算每个item出现的次数itemCount，则需要利用公式转化:由支持度计算公式Support(X) = Xs / N( N是总的交易数，Xs就是Item X的count)得， Xs = Support(X) N = itemFreq(sum(basketSize)/sum(itemFreq)) 若想查看每条购物记录中item大于指定次数的记录(只关心购买3件及以上商品的交易)，可采用如下方式过滤筛选:1groceries_use &lt;- groceries[basketSize &gt; 2] 可以采用直观的图像展示界面显示item的support值,函数如下1itemFrequencyPlot(groceries, support=0.1) #支持度大于0.1的item 1itemFrequencyPlot(groceries, topN=10, horiz=T) #支持度前10的item arules自带的inspect函数可以查看具体的数据细节问题1inspect(groceries[1:5]) #前5行数据 可以通过图形更直观观测数据的稀疏情况，一个点代表在某个transaction上购买了item。1image(groceries[1:100]) #前100行数据的展示 当数据集很大的时候，这张稀疏矩阵图是很难展现的，一般可以用sample函数进行采样显示。1image(sample(groceries,100)) #sample函数随机抽取100行数据展示 这个矩阵图虽然看上去没有包含很多信息，但是它对于直观地发现异常数据或者比较特殊的Pattern很有效。比如，某些item几乎每个transaction都会买。 比如，圣诞节都会买糖果礼物。那么在这幅图上会在糖果这一列上，显示一根竖线。 关联规则挖掘首先，我们尝试对apriori()函数以最少的限制，来观察它可以反馈给我们那些信息，再以此决定下一步操作。这里先将支 持度的最小阈值(minsup)设置为0.001，置信度最小阈值 (minconf)设为0.5，minlen=2，其他参数不进行设置取默认值，并将关联规则命名为rules0。 初探数据使用summary函数可以查看规则的汇总信息，其输出结果分为四部分。 第一部分:规则的总数(5668) 第二部分:规则的长度分布，即minlen到maxlen之间的分布。如上例，len=2有11条规则，len=3有1461条，……，len=6有46条。同时显示了rule length的五数分布+均值。 第三部分:quality measure的统计信息(支持度、置信度、提升度、支持度计数)。 第四部分:挖掘的相关信息(数据集、行数、最小支持度、最小置信度)。使用inpect函数可以查看具体的规则。 rules0包含5668条关联规则，可以想象，若将如此大量的关联规则全部输出是没有意义的。仔细观察每条规则，发现关联规则的先后顺序与可以表明其关联性的三个参数值( support、confidence、lift，count 与support均表示支持度)的取值大小没有明显关系。 面对杂乱无章的大量信息，我们无法快速获取如关联性最强等重要信息。因此，可以考虑选择生成其中关联性较强的若干条规则。 规则评估与强度控制规则可以划分为3大类: Actionable(可操作的) 这些rule提供了非常清晰、有用的洞察，可以直接应用在业务上。 Trivial(微不足道的) 这些rule显而易见，很清晰但是没啥用。属于common sense，如{尿布} =&gt; {婴儿食品}。 Inexplicable(费解的) 这些rule是不清晰的，难以解释，需要额外的研究来判定是否是有用的rule。接下来，我们讨论如何发现有用的rule。 最常用的方法是通过支持度或(和)置信度的值来实现规则控制，往往是一个不断调整的过程。最终关联规则的规模大小或者说强度高低，是根据使用者的需要决定的。但需要知道，如果阈值设置较高，容易丢失有 用信息，设置较低，则会生成大量规则。 一般地，我们有两种调整方式: 一是先使用默认参数(支持度为0.1，置信度为0.8)生成规则，然后进一步调整 二是将阈值设定很低(如rules0)，然后逐步提高阈值，直至获得设想的规则规模或强度。 通过支持度与置信度共同控制将支持度提高至0.005，置信度从0.5逐渐提高至0.64:12345rules1 &lt;- apriori(groceries, parameter = list(support =0.005,confidence = 0.5,minlen = 2))rules2 &lt;- apriori(groceries, parameter = list(support =0.005,confidence = 0.6,minlen = 2))rules3 &lt;- apriori(groceries, parameter = list(support =0.005,confidence = 0.64,minlen = 2))rules3 #显示rules3中生成关联规则条数inspect(rules3) 在两参数共同调整的过程中，如果更关注关联规则项集在总体中所占比例，则可以适当多提高支持度的值;若更注重规则本身可靠性，则可以多提高一些置信度的值。 主要通过支持度来控制也可以采取对其中一个指标给予固定阈值，再按照其他指标来选择TOP N关联规则。例如我们需要保证置信度最低为0.5，则支持度前5强的关联规则为:12rules.sorted_sup &lt;- sort(rules0,by=&quot;support&quot;) #保证置信度最低为0.5，按支持度排序inspect(rules.sorted_sup[1:5]) #输出rules.sorted_sup的前5强关联规则 主要通过置信度来控制保证支持度最低为0.001，则置信度前5强的关联规则为:12rules.sorted_sup &lt;- sort(rules0,by=&quot;support&quot;) #保证置信度最低为0.5，按支持度排序inspect(rules.sorted_sup[1:5]) #输出rules.sorted_sup的前5强关联规则 输出结果中，5条关联规则的置信度均为100%，表明只要买了关联规则前件中的商品，一定会购买后件中的。 例如第一条关联规则:购买了米和糖的消费者，都购买了全脂牛奶。这就是一条相当有用的关联规则，正如这些食品在超市中往往摆放的很近。 主要通过提升度来控制保证支持度最低为0.001，置信度最低为0.5，按lift值进行升序排序，得前5强的关联规则为:12rules.sorted_lift &lt;- sort(rules0,by=&quot;lift&quot;) #保证支持度最低为0.001，置信度最低为0.5，按置提升度排序inspect(rules.sorted_lift[1:5]) #输出rules.sorted_lift的前5强关联规则 根据前面的理论知识，我们知道，提升度用来表明规则的可靠性，提升度较高的关联规则往往是有用的。输出结果中，{即食食品，苏打水} =&gt;{汉堡肉}是强度最高的关联规则， {苏打水，爆米花} =&gt;{咸味食品}次之。 搜索规则使用subset函数，利用正则表达式，可以搜索特定规则:12yogurtrules &lt;- subset(rules1, items %in% c(&quot;yogurt&quot;))inspect(yogurtrules) %in%是精确匹配 %pin%是部分匹配，也就是说只要item like ‘%A%’ or item like‘%B%’ %ain%是完全匹配，也就是说itemset has ‘B’ and itemset has ‘B’ 同时可以通过 条件运算符(&amp;, |, !) 添加support, confidence, lift的过滤条件。 搜索规则例子:12fruitrules &lt;- subset(rules1, items %pin% c(&quot;fruit&quot;)) #搜索名称中含有“fruit”的规则-部分匹配inspect(fruitrules) 12byrules &lt;- subset(rules0, items %ain% c(&quot;berries&quot;, &quot;yogurt&quot;)) #搜索 含有“浆果”和“酸奶”的规则-完全匹配inspect(byrules) 12fruitrules1 &lt;- subset(rules1, items %pin% c(&quot;fruit&quot;) &amp; lift &gt; 3) #搜索 名称中含有“fruit”且提升度大于3的规则inspect(fruitrules1) 限制挖掘的item超市中经常出现两种商品捆绑销售的情况，这可能是商家想要促销其中某种商品。如何捆绑能够促进销量?也可以根据关联规则给出建议。 例如我们需要促销比较冷门的商品——芥末(mustard)，可 以通过设置参数appearance来控制规则的关联结果(rhs)为 “mustard”，搜索规则中仅包含mustard的关联规则，从而有效找到其强关联商品，来作为捆绑商品，但尽量要放低支持度和置信度。 代码如下:123rules4 &lt;- apriori(groceries, parameter = list( support = 0.001,confidence = 0.1, maxlen=2 ), appearance = list(rhs = c(&quot;mustard&quot;),default=&quot;lhs&quot;))#仅生成关联结果中含有“芥末”的关联规则inspect(rules4) 之所以设置maxlen=2，是因为实际情形中，我们一般仅将两种商品进行捆绑，而不是一堆商品。结果显示蛋黄酱(mayonnaise)是芥末(mustard)的强关联商品，因此我们考虑将它们捆绑起来摆放在货架上，并制定一个合适的捆绑价格。 关联规则可视化12rules5 &lt;- apriori(groceries, parameter = list( support = 0.002, confidence = 0.5)) #生成关联规则rules5 #显示rules5生成关联规则条数 1plot(rules5) #对rules5作散点图 程序运行所得图中，每个点对应于相应的支持度和置信度值， 分别由图中的横纵坐标轴显示，且其中关联规则点的颜色深浅由lift值得高低决定 另外也可以通过更改参数设置，来改变横纵轴及颜色对应的变量，如:1plot(rules5,measure=c(&quot;support&quot;,&quot;lift&quot;),shading=&quot;confidence&quot;) #更改坐标轴以及颜色渐变变量 从图中，我们可以清楚地发现以下信息: 大部分规则的support在0.1以内，Confidence在0.5-0.8内。 提升度较高的关联规则的支持度往往较低 置信度与支持度具有明显反相关性 1plot(rules5,interactive = T) #互动散点图 另外，还可以将shading参数设置为“order”来绘制出一种特殊的散点图——Two-key图。1plot(rules5,shading = &quot;order&quot;,control = list(main = &quot;Two-key plot&quot;)) #Two-key图 该图中横纵轴依旧是支持度与置信度，而关联规则点颜色的深浅则表示其所代表的关联规则中含有商品的多少，商品种类越多，点的颜色越深。 将图形类型改为“grouped”可生成组合矩阵图。1plot(rules5,method = &quot;grouped&quot;) #组合矩阵图 图中，圆点矩阵的横轴表示关联规则的前件(lhs)，纵轴表示后件(rhs)，圆点颜色越深，表示关联规则提升度(lift) 越大 圆点尺寸越大，表示关联规则支持度越高12rules5_sup &lt;- sort(rules5,by=&quot;support&quot;)inspect(rules5_sup[1:5]) 12rules5_lift &lt;- sort(rules5,by=&quot;lift&quot;)inspect(rules5_lift[1:5]) 将图形类型改为“graph”可生成关联规则图1plot(rules3, measure=&quot;confidence&quot;,method=&quot;graph&quot;,shading = &quot;lift&quot;) #关联规则图 图中，每个圆点代表一条关联规则，箭头方向标明了关联规则的lhs与rhs 1inspect(rules3)","categories":[{"name":"技术知识","slug":"技术知识","permalink":"http://www.ilmadyna.com/categories/技术知识/"},{"name":"数据分析","slug":"技术知识/数据分析","permalink":"http://www.ilmadyna.com/categories/技术知识/数据分析/"}],"tags":[{"name":"数据分析","slug":"数据分析","permalink":"http://www.ilmadyna.com/tags/数据分析/"},{"name":"关联规则","slug":"关联规则","permalink":"http://www.ilmadyna.com/tags/关联规则/"},{"name":"Apriori","slug":"Apriori","permalink":"http://www.ilmadyna.com/tags/Apriori/"},{"name":"R","slug":"R","permalink":"http://www.ilmadyna.com/tags/R/"}]},{"title":"电商数据分析之基本指标体系","slug":"电商数据分析之基本指标体系","date":"2018-05-09T06:33:08.476Z","updated":"2018-05-12T11:07:17.540Z","comments":true,"path":"2018/05/09/电商数据分析之基本指标体系/","link":"","permalink":"http://www.ilmadyna.com/2018/05/09/电商数据分析之基本指标体系/","excerpt":"","text":"总体运营指标从流量、订单、总体销售业绩、整体指标进行把控，起码对运营的电商平台有个大致了解，到底运营的怎么样，是亏是赚。 流量类指标独立访客数（UV)独立访客就是独立（IP）访客（Unique Visitor）一般地，我们可以用两个数值标准来统计访问某网站的访客，即“访问次数”和“独立访客（问）数”，访问次数和独立访客数是两个不同的概念。 定义举例说明访问次数就相当于一个展览会的访问人次，某个参观者出入展馆10次的话，这10次都被记入访问次数中，相当于网络中的PV值。 独立访客数则相当于带身份证参观展览会的访问人数，每一个出示身份证参观展览的人，无论出入几次，都只计作一次独立访问。这里所说的“身份证”，在网络上就是访客的IP地址或Cookie。 技术依据网站统计工具对于网站独立访客数的计算，主要是依据浏览器的“cookie” 来判定的。在浏览器cookie数据不清除的情况下，即使用多个IP切换来登录一个网站，也会只记为一个访客数。 独立访客很接近但并不完全就是真实独立的人。其次，独立访客这个指标会受浏览器设置的影响，如那些将浏览器设置成禁用cookie或是禁用第三方cookie的情况。目前大多数的网站分析工具都使用第一方cookie来尽量降低cookie被禁用的情况(被禁用百分比大概在2%至5%之间)。第三方cookie被禁用的比率相对而言就要高很多(大概在10%至30%之间)。 计算规则记录独立访客数的时间标准一般可为一天，一个月。按照国际惯例，独立访客数记录标准一般为“一天”，即一天内如果某访客从同一个IP地址来访问某网站n次的话，访问次数计作n, 独立访客数则计作1。一般不计算年UV数。 独立IP区别独立访客指的是独立上网的电脑，与唯一IP不同，独立访客高于IP数的时候正常是以下这种状况：例如一个局域网对外是相同的一个IP，但是有10个人同时访问，那么这个时候，独立访客为10，唯一IP仅为1；IP高于独立访客的时候正常是以下这种状况：一个用户，上网的时候频繁掉线，拔号10次均打开了受统计网站，此时，独立访客仅计为1，而IP数则被计为10。 联系IP是英文Internet Protocol（网络之间互连的协议）的缩写，中文简称为“网协”，也就是为计算机网络相互连接进行通信而设计的协议。在因特网中，它是能使连接到网上的所有计算机网络实现相互通信的一套规则，规定了计算机在因特网上进行通信时应当遵守的规则。任何厂家生产的计算机系统，只要遵守 IP协议就可以与因特网互连互通。IP地址具有唯一性，根据用户性质的不同，可以分为5类。另外，IP还有进入防护，知识产权，指针寄存器等含义。 区分对于ADSL上网的用户：一个用户有可能在一天之内多次重复访问某站点，如果他在同一台电脑上，并且IP没有变化的情况下，那么这个用户为该站点贡献了一个独立IP数，一个独立访客数，而PV则是随着他的浏览在不断增加；如果他在中途在不改变电脑的情况下，掉线了一次，重新连上后IP发生了变化，之后他又访问了该站点，那么这个用户为该站点贡献了两个IP数，一个独立访客数。 对于网吧、单位、学校上网的用户：这些场所一般都是采用局域网共享上网的方式，只有一个IP地址接入互联网，此种情况就有可能出现独立访客数大于独立IP数，比如某单位内有多人访问A站点，但是整个单位的公网IP出口就只有一个，那么无论该单位里在当日有多少人次访问A站点，这些用户为A站点贡献的独立IP数为1，除非该单位的IP地址发生了变化，相反，这些用户为A站点带来的独立访客数就是该单位内当日访问A站点的实际人数了。 由此可见，独立访客比独立IP更具说服力，只不过我们平时比较关心独立IP罢了。一个网站的独立IP数与独立访客数是相近的，有可能独立IP数&gt;独立访客数，也有可能独立IP数&lt;独立访客数，当然也有可能是相等的，这取决于网站的用户访问情况。而PV则是永远都是大于等于独立IP和独立访客的，因为PV是重复统计的。 相关术语在流量统计系统里面我们经常会见到诸如独立IP、独立访客、PV之类的术语，对于这三者间的含义和关系，总会让人产生一些疑惑和不解。 术语定义PV（访问量） 即Page View，页面浏览量或点击量，用户每次刷新即被计算一次。指某站点总共有被浏览多少个页面，它是重复累计的，同一个页面被重复浏览也被计入PV。 UV（独立访客量） 即Unique Visitor，独立访客是指某站点被多少台电脑访问过，以用户电脑的Cookie作为统计依据。00:00-24:00内相同的客户端只被计算一次。 IP（独立IP） 即Internet Protocol，独立IP是指访问过某站点的IP总数，以用户的IP地址作为统计依据。00:00-24:00内相同IP地址只被计算一次。 区别联系PV（访问量） PV高不一定代表来访者多；PV与来访者的数量成正比，但是PV并不直接决定页面的真实来访者数量。比如一个网站就你一个人进来，通过不断的刷新页面，也可以制造出非常高的PV。 UV（独立访客量） UV是指不同的、通过互联网访问、浏览一个网页的自然人。比如，在一台电脑上，哥哥打开了微软的官方主页，注册了一个会员。弟弟一会儿也看了看，注册了另一个会员。由于兄弟两个使用的是相同的计算机，那么他们的 IP是一样的，微软的官方计数器记录到一个IP登陆的信息。但是，具有统计功能的统计系统，可以根据其他条件判断出实际使用的用户数量，返回给网站建设者真实、可信和准确的信息。比如通过注册的用户，甚至可以区分出网吧、机房等共享一个IP地址的不同计算机。 上面的例子就说明虽然是同一IP，但是有2个独立访客。再举个例子吧，比如一个网吧里，有100个人都进入了我的网站，但是一个网吧对外都是一个IP的，所以统计系统只统计到一个IP；但是因为网吧里有100人在访问我的站，尽管他们都仅仅打开我的网站的首页，或者这100人都把我网站所有页面都看过了一遍，统计系统都只统计到100个独立访客。 IP（独立IP） 表示拥有特定唯一IP地址的计算机访问您的网站的次数，因为这种统计方式比较容易实现，具有较高的真实性，所以成为大多数机构衡量网站流量的重要指标。比如你是ADSL拨号上网的，你拨一次号都自动分配一个IP，这样你进入了本站，那就算一个IP，当你断线了而没清理Cookies，之后又拨了一次号，又自动分配到一个IP，你再进来了本站，那么又统计到一个IP，这时统计数据里IP就显示统计了2次。但是UV（独立访客）没有变，因为2次都是你进入了本站。 IP在这里是指公用的广域网传输协议族（Tcp/Ip）为每一台处在因特网上的计算机（可以是个人电脑、服务器以及其他兼容广域网传输协议族规定的 接入设备）都定义了四个段落（例如：192.168.0.255形式，有时会加入第五段落端口号作为描述信息，端口号是介于1-65535之间的数字）共 32位长度二进制代码的标识，叫IP协议地址，简称IP地址，俗称IP，它是一个一台连接着广域网的计算机区别于其他机器的标识，一般情况下，它在同一级别的网络（例如某个局域网、社区网、教学楼网或者INTERNET）范围内是唯一的。 页面访客数（PV)页面访问量是指访客进入某网站后，浏览过的该网站网页的数量。 基本介绍页面访问量(Page View) 页面访问量是访客实际浏览过的网页数的总和，而不是某网站中的网页数的总和。 PV(page view) 即页面浏览量，或点击量;通常是衡量一个网络新闻频道或网站甚至一条网络新闻的主要指标。根据百度统计后台的官方解释：PV即通常说的Page View，用户每打开一个网站页面就被记录1次。用户多次打开同一页面，浏览量值累计。 PV之于网站，就像收视率之于电视，从某种程度上已成为投资者衡量商业网站表现的重要尺度。 提升数量方法搜索引擎优化SEO即搜索引擎优化，是通过对网站网络营销结构(内部链接结构、网站物理结构、网站逻辑结构)、高质量的网站主题内容、丰富而有价值的相关性外部链接进行优化而使网站为用户及搜索引擎更加友好，以获得在搜索引擎上的优势排名为网站引入流量。 PPC竞价推广（例：百度竞价）PPC，是指购买搜索结果页上的广告位来实现营销目的，各大搜索引擎都推出了自己的广告体系，相互之间只是形式不同而已。搜索引擎广告的优势是相关性，由于广告只出现在相关搜索结果或相关主题网页中，因此，搜索引擎广告比传统广告更加有效，客户转化率更高。 参与博客论坛等社区虽然论坛博客等地方都不允许发广告性的东西或垃圾链接，但只要你能够提供有用的信息，对社区有贡献，比如回答其他人的问题，很多浏览的人会把你当作专家，点击你签名中的链接。搜索引擎也同样。虽然搜索引擎给论坛博客里的链接的权重都很低，但积少成多。 发表文章写出文章后，不仅发在自己的网站上，也可以发到其他接受客座作者文章的网站和电子杂志等。英文网站中有不少是专门收集这些文章的，其他的站长也会到这些文章收集网站来寻找有用的东西，放在自己的网站或电子杂志里。这些文章里面的作者信息都会包含指向原出处的链接。 提供免费有用的线上服务你可以写一个小程序，放在自己的网站上。如果这个小程序真的有用的话，其他有共同爱好的人很自然地会链接向你，比如我们经常看到的在线查询PR的程序、查询关键词密度的在线工具、计算自己是否超重的小工具等。 免费下载有用的东西不是指免费下载小电影、非法盗用的源程序等，而是独特的自己写出来的东西。比如免费电子书，免费博客模板，免费博客或论坛插件。这是非常有效的方法。其实很多博客和论坛软件本身，像WordPress不会交换链接，但是使用这些软件的网站很自然会回报善意而链接向它。有时侯免费使用的要求就是保留原作者链接，比如这个博客的模板。 提交分类目录两个重要的网站分类目录雅虎和开放目录对网站排名，特别是克服Google Sandbox沙盒效应还是有很大作用的。不过进入这两个目录挺困难，雅虎英文目录收取每年299美金的审查费，却不保证收录。雅虎中文目录似乎已经被取消了。开放目录是由志愿编辑来审查网站的，有的时候编辑处理很慢，有的时候会带有一些偏见。还有不少行业性或地区性的网站分类目录，都尽量逐渐向这些目录申请登录。 向个人网站寻求链接由于互联网刚开始的时候没什么商业性，很多很个人的网页历史非常早，PR值很高，被信任度也很高，尤其是一些大学或非盈利组织的网站中的网页。很多大学老师都在所在学校域名有专门的网页，有很多学生也都有建在学校域名的网页。而不少大学网站PR值和信任度都相当高，这些比较个人性质的和研究性质的网页也都有很高的链接投票权重。如果你找到和你的行业相近的这类网站，不妨直接和网页的主人联系，如果你有一个内容丰富的网站，看对方能不能给你一个外部链接。 买链接虽然搜索引擎对买卖链接，尤其是以PR为目的的买卖链接，非常不喜欢，如果被检测到是买卖的链接，一般链接的投票权重都会消失，但毕竟买卖的链接和自然的链接的判断是很困难的。搜索引擎怎么从链接本身来知道私下有金钱交易呢？当然在购买链接的时候，应该避免那些经常被判断为买卖链接的特征。比如说在链接周围有广告赞助等字样，链接出现在左面的菜单下面，买链接的网站和你的行业完全无关，整个网站每个网页都链接向你等等。 交换链接也就是我链接向你，你链接向我。有时候不少站长用三向的间接交换链接，其实这种三向的模式对搜索引擎来说并不难判断。虽然交换链接的价值越来越低，尤其对Google来说。但是两个相关的网站，或者是好朋 友的站长之间交换链接是很正常很自然的一件事情。只要内容相关，交换链接在一定时期内不会完全没有作用。 人均页面访问数人均访问页面数与PV/UV不一样。 人均访问页面数的计算公式是session所记录访问数/UV。session所记录访问数是通过浏览器访问的session来记录的访问数，有些访问行为是记录不到session，所以这种记录不到session的访问行为不会被放到session所记录访问数中，但会出现在PV中。也就是说，session所记录访问数一般情况下小于等于PV，也就导致人均访问页面数要小于等于PV/UV。 订单产生效率指标总订单数访问到下单转化率总体销售业绩指标成交金额销售金额客单价整体指标销售毛利毛利率网站流量指标既对访问你网站的访客进行分析，基于这些数据可以对网页进行改进，以及对访客的行为进行分析等等。 流量规模类指标独立访客数（UV）页面访客数（PV）流量成本类指标访客获取成本流量质量类指标跳出率页面访问时长人均页面访问数会员类指标注册会员数活跃会员数活跃会员率会员复购率会员平均购买次数会员回购率会员留存率销售转化指标分析从下单到支付整个过程的数据，帮助你提升商品转化率。也可以对一些频繁异常的数据展开分析。 购物车类指标购入购物车次数加入购物车买家次数加入购物车商品数购物车支付转化率下单类指标下单笔数下单金额下单买家数浏览下单转化率支付类指标支付金额支付买家数支付商品数浏览-支付宝买家转化率下单-支付宝金额转化率下单-支付时长交易类指标交易成功订单数交易成功交易金额交易成功买家数交易成功商品数交易失败订单数交易失败订单金额交易失败订单买家数交易失败商品数退款总订单量退款金额退款率客户价值指标这里主要就是分析客户的价值，可以建立RFM价值模型，找出那些有价值的客户，精准营销等等。 客户指标累积购买客户数客户单价新客户指标新客户数量新客户获取成本新客户单价老客户指标消费频率最近一次购买时间消费金额重复购买次数商品类指标主要分析商品的种类，那些商品买得好，库存情况，以及可以建立关联模型，分析那些商品同时销售的几率比较高，而进行捆绑销售，有点像啤酒和尿布的故事。 产品总数指标SKU数SPU数在线SPU数产品优势性指标独家产品收入比重品牌存量品牌数在线品牌数上架上架商品SKU数上架商品SPU数上架在线SPU数上架商品数上架在线商品数首发首次上架商品数首次上架在线商品数市场营销活动指标主要监控某次活动给电商网站带来的效果，以及监控广告的投放指标。 市场营销活动指标新增访问人数新增注册人数总访问次数订单数量下单抓换率ROI（投资回报率）广告投放指标新增访客数新增注册人数总访问次数订单数量UV订单转化率广告投资回报率风控类指标分析卖家评论，以及投诉情况，发现问题，改正问题。 买家评价指标买家评价数买家评价卖家数买家评价上传图片数买家评价率买家好评率买家差评率投诉类指标发起投诉（申诉）数投诉率撤销投诉（申诉）数市场竞争指标主要分析市场份额以及网站排名，进一步进行调整。 市场份额相关市场占有率市场扩大率用户份额网站排名交易额排名流量排名","categories":[{"name":"技术知识","slug":"技术知识","permalink":"http://www.ilmadyna.com/categories/技术知识/"},{"name":"数据分析","slug":"技术知识/数据分析","permalink":"http://www.ilmadyna.com/categories/技术知识/数据分析/"}],"tags":[{"name":"电商","slug":"电商","permalink":"http://www.ilmadyna.com/tags/电商/"},{"name":"电商数据分析","slug":"电商数据分析","permalink":"http://www.ilmadyna.com/tags/电商数据分析/"}]}]}